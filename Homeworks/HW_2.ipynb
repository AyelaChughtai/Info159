{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp21/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KuVSCSqlUX",
        "outputId": "5c58008e-e693-4bbe-a469-99c4d4b99f7c"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll use to make predictions for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn0XtfFeqP2P",
        "outputId": "8254b3b5-561e-4306-ff15-29479feee7d2"
      },
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/test.txt"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 04:33:58--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt.5’\n",
            "\n",
            "train.txt.5         100%[===================>]   1.26M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-02-02 04:33:58 (15.2 MB/s) - ‘train.txt.5’ saved [1322055/1322055]\n",
            "\n",
            "--2021-02-02 04:33:58--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt.5’\n",
            "\n",
            "dev.txt.5           100%[===================>]   1.25M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-02-02 04:33:58 (17.4 MB/s) - ‘dev.txt.5’ saved [1309909/1309909]\n",
            "\n",
            "--2021-02-02 04:33:58--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt.5’\n",
            "\n",
            "test.txt.5          100%[===================>]   6.27M  25.7MB/s    in 0.2s    \n",
            "\n",
            "2021-02-02 04:33:59 (25.7 MB/s) - ‘test.txt.5’ saved [6573426/6573426]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "*First*, let's define a classifier based on a really simple dictionary-based feature: if the abstract contains the words \"love\" or \"like\", the CONTAINS_POSITIVE_WORD feature will fire, and if it contains either \"hate\" or \"dislike\", the CONTAINS_NEGATIVE_WORD will fire.  Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.  Note the `L2_regularization_strength` specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and the `min_feature_count` specifies how many data points need to contain a feature for it to be allowable as a feature in the model.  Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnqjxd6fKPiP",
        "outputId": "925ea3fe-cd6a-4241-e256-7732ecb92e9e"
      },
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "First, is this accuracy score any good?  Let's calculate the accuracy of a majority class predictor to provide some context.  Again, this determines the most represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t--LfOjPj7T",
        "outputId": "c19ef801-f6e7-4e76-98ce-3dab5db56203"
      },
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "    \n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "            \n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "# Your assignment\n",
        "\n",
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm three additional distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data. \n",
        "\n",
        "Describe your features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.  \n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Dev set performance|\n",
        "|---|---|---|\n",
        "|Bag of words| For bag of words, I simply tokenized every ‘word’ in the text. This includes stop words like ‘the’ and punctuation marks that don’t really help us differentiate between positive and negative reviews. Each ‘word’ was made into a feature, and this heavily overfit the data as evidenced by the perfect training score and the relatively low test score. |0.771|\n",
        "|Feature 1| I thought that stop words and punctuation were bringing the dev score down as they do not indicate positive OR negative sentiment generally, so I removed them from the list of tokenized words and only created features out of the resulting ‘real’ words. I think the real words commonly used in positive and negative reviews were distinct enough that the dev score increased slightly.|0.773|\n",
        "|Feature 2| There are some words such as ‘like’ that may be common in positive AND negative reviews, but in negative reviews they would be preceded by a negating word producing a bigram like ‘not like’. Thus, I though that using bigrams (also without stop words and punctuation) may help us differentiate between these cases, although it did not have a high dev score. |0.704|\n",
        "|Feature 3| Similar to why I used bigrams, I thought there would be cases where an adjective was used with a positive word like ‘very good’ and a negating word might precede it like ‘not very good’. Thus, I used trigrams (also without stops and punctuation) to differentiate between such cases. Even though the resulting dev score was the lowest of the 3 features, it did bring the overall dev accuracy up slightly (by 0.002). |0.564|\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  The most creative features (defined as features that few other students use and that are reasonably well-performing) will receive extra credit for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        feats[word] = 1\n",
        "\n",
        "    return feats"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3AJ5qMBeqmL",
        "outputId": "74a05b43-eb1d-4526-8b27-0cf4f388ead4"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: bag_of_words, Features: 21224, Train accuracy: 1.000, Dev accuracy: 0.771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocPMYhIt4BX0",
        "outputId": "abc104d1-8136-441b-9ae8-f07f01a8f2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    # remove stop words, punctuation, and white spaces and make the rest of the words features\n",
        "\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    puncs = [ ',', '<', '>', '.', '/', '?', ';', ':', \"'\", '\"', '[', ']', '{', '}', \n",
        "    ' \\ ', '|',  '~', '`', '!', '#', '@', '$', '%', '^', '&', '*', '(', ')', ' ']\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if (not word in stopwords) and (not word in puncs):\n",
        "          feats[word] = 1\n",
        "               \n",
        "    return feats"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MAwRwbQ7lVw",
        "outputId": "f99b6baf-1d78-4370-efd8-63c2f4e7f5af"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: feature1, Features: 21069, Train accuracy: 1.000, Dev accuracy: 0.773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "source": [
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    # remove punc and spaces and make bigrams features\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    puncs = [ ',', '<', '>', '.', '/', '?', ';', ':', \"'\", '\"', '[', ']', '{', '}', \n",
        "    ' \\ ', '|',  '~', '`', '!', '#', '@', '$', '%', '^', '&', '*', '(', ')', ' ']\n",
        "    words = [word for word in words if word not in stopwords and word not in puncs]\n",
        "    bigrams = nltk.bigrams(words)\n",
        "    for bigram in bigrams:\n",
        "      feats[bigram] = 1\n",
        "    return feats"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgpuykF67oWZ",
        "outputId": "12d3460b-6ec1-40d9-cffa-e6503d584aa0"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: feature2, Features: 108547, Train accuracy: 1.000, Dev accuracy: 0.704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    # remove punc and spaces and make trigrams features\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    puncs = [ ',', '<', '>', '.', '/', '?', ';', ':', \"'\", '\"', '[', ']', '{', '}', \n",
        "    ' \\ ', '|',  '~', '`', '!', '#', '@', '$', '%', '^', '&', '*', '(', ')', ' ']\n",
        "    words = [word for word in words if word not in stopwords and word not in puncs]\n",
        "    trigrams = nltk.trigrams(words)\n",
        "    for trigram in trigrams:\n",
        "      feats[trigram] = 1\n",
        "    return feats"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_f--utb7q4l",
        "outputId": "3011a8f9-28bb-463d-b2a3-7417155663fb"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: feature3, Features: 125575, Train accuracy: 1.000, Dev accuracy: 0.564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  all_feats={}\n",
        "  for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "    all_feats.update(feature(text))\n",
        "  return all_feats"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-tRUFTIdAqT",
        "outputId": "4fe7b5dd-77f7-4f01-9c93-ae5299db31a6"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: combiner_function, Features: 255346, Train accuracy: 1.000, Dev accuracy: 0.786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        " ## Deliverable 2\n",
        "\n",
        "This code will generate a file named `combiner_function_predictions.csv`; download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook; the 5 systems with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "## Interrogating classifiers\n",
        "\n",
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  **Note that nothing below this line requires any work on your part; treat these as useful tools for understanding what works and what doesn't.**\n",
        "\n",
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels).  What kinds of mistakes is it making?  (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "7ulxd1TosIMV",
        "outputId": "a98b0deb-9eb6-4585-ad69-64ff91f9e924"
      },
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most defining for each of the classes (ranked by how strong their corresponding coefficient is).  Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAyGuXIi9pqe",
        "outputId": "44ca976f-7ce3-4801-c816-85c544a2467b"
      },
      "source": [
        "big_classifier.printWeights(n=25)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\t0.653\tgreat\n",
            "pos\t0.495\tvery\n",
            "pos\t0.424\tbest\n",
            "pos\t0.411\texcellent\n",
            "pos\t0.348\tman\n",
            "pos\t0.322\tfun\n",
            "pos\t0.320\tmy\n",
            "pos\t0.320\talso\n",
            "pos\t0.312\tamazing\n",
            "pos\t0.309\tenjoyed\n",
            "pos\t0.294\tyou\n",
            "pos\t0.287\tperfect\n",
            "pos\t0.283\tnew\n",
            "pos\t0.277\tsimple\n",
            "pos\t0.265\trelationship\n",
            "pos\t0.258\tin\n",
            "pos\t0.257\tperformance\n",
            "pos\t0.245\t!\n",
            "pos\t0.241\tlove\n",
            "pos\t0.241\tsaw\n",
            "pos\t0.236\ttrue\n",
            "pos\t0.235\tseries\n",
            "pos\t0.235\t'\n",
            "pos\t0.233\twill\n",
            "pos\t0.231\tincredible\n",
            "\n",
            "neg\t-0.617\tworst\n",
            "neg\t-0.549\tacting\n",
            "neg\t-0.547\teven\n",
            "neg\t-0.482\twould\n",
            "neg\t-0.436\t?\n",
            "neg\t-0.431\tbad\n",
            "neg\t-0.406\tstupid\n",
            "neg\t-0.404\tboring\n",
            "neg\t-0.400\twaste\n",
            "neg\t-0.390\tno\n",
            "neg\t-0.353\tactors\n",
            "neg\t-0.327\twatching\n",
            "neg\t-0.316\tnothing\n",
            "neg\t-0.315\tidea\n",
            "neg\t-0.315\tme\n",
            "neg\t-0.308\tthere\n",
            "neg\t-0.305\twere\n",
            "neg\t-0.305\thorrible\n",
            "neg\t-0.304\tpoor\n",
            "neg\t-0.297\tmoney\n",
            "neg\t-0.296\tdo\n",
            "neg\t-0.293\tthat\n",
            "neg\t-0.293\tn't\n",
            "neg\t-0.285\tawful\n",
            "neg\t-0.280\tcould\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are most mistaken. Does it suggest any features you might create to disentangle them?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UXmRhSuzxaJi",
        "outputId": "26efbdf3-191d-4d8a-a798-4e0c30272881"
      },
      "source": [
        "analyze(big_classifier)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1436</td>\n",
              "      <td>0.999602</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1784</td>\n",
              "      <td>0.971246</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>The only complaint I have about this adaptation is that it is sexed-up. Things that were only hinted at in the novel are shown on-screen for some weird reason. Did they think the audience would be too stupid to understand if they were not shown everything out-right? Other than that, this is very good-quality. All the actors do marvelous jobs bringing their characters to life. For the shallow w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1456</td>\n",
              "      <td>0.969125</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't too terrible but it wasn't that much better than average. The main story dealing with highly immoral teens should have focused more on the forbidden romance and why this was... should have really gotten into it instead of scraping the surface with basically \"because mom says we can't.\" Some parts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1206</td>\n",
              "      <td>0.958258</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Plot Synopsis: Hong Kong, 1966. Paul Wagner, the man who built the Victoria Tunnel, is murdered along with his wife by his associates. His twin sons, Chad &amp; Alex, are split apart. 25 years later, Chad, a karate instructor in Los Angeles, &amp; Alex, a smuggler living in Hong Kong, join forces to avenge their parents' murder &amp; rightfully claim the tunnel.This is the second time that Jean-Claude Van...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1018</td>\n",
              "      <td>0.957892</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This was surprisingly intelligent for a TV movie, and quite true to my own experience of bulimia. It was actually well-researched, and I can only assume it was written by someone who's gone through a similar experience, because it had all the little details. The characters were quite well-drawn, and the performances by Mare Winningham and Alison Lohman were great. I think what I like most was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1608</td>\n",
              "      <td>0.956102</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant internal turmoils it concerns itself with the Emperor Franz Joseph (Mr James Mason),his rebellious son,the Crown Prince Rudolf (Mr Omar Sharif)the Empress(Miss Ava Gardner) and various mistresses,secret ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1825</td>\n",
              "      <td>0.951981</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Whilst reading through the comments left for this show, I couldn't help but notice that a large percentage of the reviewers had either not actually watched any episodes of the show either all the way through or of their own free will. The thing about Kerching! is that it's a children's show, FOR CHILDREN so obviously if your older it is going to seem cheesy, forced, and probably stupid. I even...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1787</td>\n",
              "      <td>0.949032</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her more after the documentary. I felt awful watching this documentary, it was like reliving the nightmare and it still brings tears to my eyes.But I'm extremely grateful that I watched this documentary,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1087</td>\n",
              "      <td>0.948158</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>That's what one of the girls said at the end.Is the soccer game a metaphor for a qualifying game between the girls (or more broadly, a free-thinking group) and the authority? \"To Germany\" means to a future that's of hope? It's one of the most unforgettable cinematic experience I've ever had -- despite the crude cinematography and plot, and mild over-acting (though I like the cast -- they're lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1982</td>\n",
              "      <td>0.947351</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>The major flaw with the film is its uninspired script. It plods back and forth between vignettes of Bettie's story and re-creations of the Klaw short films. While the Klaw re-creations are well done, it is unnecessary to recreate them in their near entirety. Page Richards, while not an amazing actress, does a decent job overall. And, at times, she does bear a remarkable resemblance to Bettie. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1822</td>\n",
              "      <td>0.946922</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1833</td>\n",
              "      <td>0.946759</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This was yet another big screen outing for a US TV show from the sixties It is amusing enough but was very much to formula. Intelligent Martian lands on Earth and meets the not too bright humans, in his view.The usual wackiness ensues with the human, Bridges, eventually bonds with him and helps him to get home. Along the way he also gets the girl, Hannah.This is a nice outing for some pleasant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1361</td>\n",
              "      <td>0.944851</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Bridges of madison county is a better made version of this story. I felt the ending of this movie was not handled sensitively as they did in the original English movie. This movie is very indianised, if you are a very sensitive person who cries in a movie when hero dies in the end you'll love this movie, On the other hand if you are a fighter in life and think crying is for wimps you may not l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1862</td>\n",
              "      <td>0.939894</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>In the previews, \"The 40 Year-Old Virgin\" boasts the image of another immature sex romp about a 40-ish Lonely Guy who suddenly feels the urge to do the deed simply because he hasn't. Too many past bad experiences have dampened his enthusiasm to the point that he avoids women completely. And then the unexpected happens: he falls in love. What's more, there's a movie out about it, and it's calle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1314</td>\n",
              "      <td>0.939514</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Chesty gringo Telly Savalas (as Frank Cooper) is a US-Mexico \"Border Cop\". He serves as a father figure to young immigrant Danny De La Paz (as Benny Romero), who wants Mr. Savalas to be best man at his impending wedding. Savalas is tough, but boss Eddie Albert (as Commander Moffat) may be tougher. Tough is what you need to stop smuggler Michael V. Gazzo (as Chico Suarez). Alliances may be in f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1464</td>\n",
              "      <td>0.938180</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I really wanted to like this film, but so much of it is stolen/borrowed from other work -- some of the borrowing is painfully blatant. The New York Times' review pointed out that their singing frog is awfully reminiscent of the one in the famous Warner Brothers' cartoon ('Hello my baby, hello my darlin', hello my ragtime gal...'). But I challenge anyone to watch the Fox/Blue Sky animated featu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1702</td>\n",
              "      <td>0.933994</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>David Arquette is a young and naive home security alarmsalesman taken under the wing of Stanley Tucci. Arquette is agolden boy, scoring a big sale on his first call- to widow KateCapshaw and her dopey son Ryan Reynolds. Things are goingwell for Arquette, he is appearing in commercials for the securityfirm and he is falling in love with Capshaw.Then Tucci and his right hand woman Mary McCormack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1043</td>\n",
              "      <td>0.932576</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Some bad reviews here for this and I understand why but treat it as a low budget serial killer film and you might get more from it than most.I thought that this worked in a way because afterwards I felt dirty and wanted to take a long shower so that is some degree of success isn't it?I would say there is just the right level of sleaze here to get under your skin although the acting is maybe a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1976</td>\n",
              "      <td>0.932207</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I'm glad that users (as of this date) who liked this movie are now coming forward. I don't understand the people who didn't like this movie - it seems like they were expecting a serious (?!?!?) treatment! C'mon, how the hell can you take the premise of a killer snowman seriously? The filmmakers knew this was a silly premise, and they didn't try to deny it. The straight-faced delivery of scenes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1471</td>\n",
              "      <td>0.923608</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any charlie brown subtitler a run for his money he just constantly mumbles which is always a laugh, most scenes just feel awkward with characters more often than not gazing across to another with a look o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                                                                                                                                                                                                                                                                                                                                                                                             Text\n",
              "0   1436  ...  I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...\n",
              "1   1784  ...  The only complaint I have about this adaptation is that it is sexed-up. Things that were only hinted at in the novel are shown on-screen for some weird reason. Did they think the audience would be too stupid to understand if they were not shown everything out-right? Other than that, this is very good-quality. All the actors do marvelous jobs bringing their characters to life. For the shallow w...\n",
              "2   1456  ...  Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't too terrible but it wasn't that much better than average. The main story dealing with highly immoral teens should have focused more on the forbidden romance and why this was... should have really gotten into it instead of scraping the surface with basically \"because mom says we can't.\" Some parts...\n",
              "3   1206  ...  Plot Synopsis: Hong Kong, 1966. Paul Wagner, the man who built the Victoria Tunnel, is murdered along with his wife by his associates. His twin sons, Chad & Alex, are split apart. 25 years later, Chad, a karate instructor in Los Angeles, & Alex, a smuggler living in Hong Kong, join forces to avenge their parents' murder & rightfully claim the tunnel.This is the second time that Jean-Claude Van...\n",
              "4   1018  ...  This was surprisingly intelligent for a TV movie, and quite true to my own experience of bulimia. It was actually well-researched, and I can only assume it was written by someone who's gone through a similar experience, because it had all the little details. The characters were quite well-drawn, and the performances by Mare Winningham and Alison Lohman were great. I think what I like most was ...\n",
              "5   1608  ...  As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant internal turmoils it concerns itself with the Emperor Franz Joseph (Mr James Mason),his rebellious son,the Crown Prince Rudolf (Mr Omar Sharif)the Empress(Miss Ava Gardner) and various mistresses,secret ...\n",
              "6   1825  ...  Whilst reading through the comments left for this show, I couldn't help but notice that a large percentage of the reviewers had either not actually watched any episodes of the show either all the way through or of their own free will. The thing about Kerching! is that it's a children's show, FOR CHILDREN so obviously if your older it is going to seem cheesy, forced, and probably stupid. I even...\n",
              "7   1787  ...  My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her more after the documentary. I felt awful watching this documentary, it was like reliving the nightmare and it still brings tears to my eyes.But I'm extremely grateful that I watched this documentary,...\n",
              "8   1087  ...  That's what one of the girls said at the end.Is the soccer game a metaphor for a qualifying game between the girls (or more broadly, a free-thinking group) and the authority? \"To Germany\" means to a future that's of hope? It's one of the most unforgettable cinematic experience I've ever had -- despite the crude cinematography and plot, and mild over-acting (though I like the cast -- they're lo...\n",
              "9   1982  ...  The major flaw with the film is its uninspired script. It plods back and forth between vignettes of Bettie's story and re-creations of the Klaw short films. While the Klaw re-creations are well done, it is unnecessary to recreate them in their near entirety. Page Richards, while not an amazing actress, does a decent job overall. And, at times, she does bear a remarkable resemblance to Bettie. ...\n",
              "10  1822  ...  Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...\n",
              "11  1833  ...  This was yet another big screen outing for a US TV show from the sixties It is amusing enough but was very much to formula. Intelligent Martian lands on Earth and meets the not too bright humans, in his view.The usual wackiness ensues with the human, Bridges, eventually bonds with him and helps him to get home. Along the way he also gets the girl, Hannah.This is a nice outing for some pleasant...\n",
              "12  1361  ...  Bridges of madison county is a better made version of this story. I felt the ending of this movie was not handled sensitively as they did in the original English movie. This movie is very indianised, if you are a very sensitive person who cries in a movie when hero dies in the end you'll love this movie, On the other hand if you are a fighter in life and think crying is for wimps you may not l...\n",
              "13  1862  ...  In the previews, \"The 40 Year-Old Virgin\" boasts the image of another immature sex romp about a 40-ish Lonely Guy who suddenly feels the urge to do the deed simply because he hasn't. Too many past bad experiences have dampened his enthusiasm to the point that he avoids women completely. And then the unexpected happens: he falls in love. What's more, there's a movie out about it, and it's calle...\n",
              "14  1314  ...  Chesty gringo Telly Savalas (as Frank Cooper) is a US-Mexico \"Border Cop\". He serves as a father figure to young immigrant Danny De La Paz (as Benny Romero), who wants Mr. Savalas to be best man at his impending wedding. Savalas is tough, but boss Eddie Albert (as Commander Moffat) may be tougher. Tough is what you need to stop smuggler Michael V. Gazzo (as Chico Suarez). Alliances may be in f...\n",
              "15  1464  ...  I really wanted to like this film, but so much of it is stolen/borrowed from other work -- some of the borrowing is painfully blatant. The New York Times' review pointed out that their singing frog is awfully reminiscent of the one in the famous Warner Brothers' cartoon ('Hello my baby, hello my darlin', hello my ragtime gal...'). But I challenge anyone to watch the Fox/Blue Sky animated featu...\n",
              "16  1702  ...  David Arquette is a young and naive home security alarmsalesman taken under the wing of Stanley Tucci. Arquette is agolden boy, scoring a big sale on his first call- to widow KateCapshaw and her dopey son Ryan Reynolds. Things are goingwell for Arquette, he is appearing in commercials for the securityfirm and he is falling in love with Capshaw.Then Tucci and his right hand woman Mary McCormack...\n",
              "17  1043  ...  Some bad reviews here for this and I understand why but treat it as a low budget serial killer film and you might get more from it than most.I thought that this worked in a way because afterwards I felt dirty and wanted to take a long shower so that is some degree of success isn't it?I would say there is just the right level of sleaze here to get under your skin although the acting is maybe a ...\n",
              "18  1976  ...  I'm glad that users (as of this date) who liked this movie are now coming forward. I don't understand the people who didn't like this movie - it seems like they were expecting a serious (?!?!?) treatment! C'mon, how the hell can you take the premise of a killer snowman seriously? The filmmakers knew this was a silly premise, and they didn't try to deny it. The straight-faced delivery of scenes...\n",
              "19  1471  ...  But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any charlie brown subtitler a run for his money he just constantly mumbles which is always a laugh, most scenes just feel awkward with characters more often than not gazing across to another with a look o...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxwwblfh9pqf"
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxdMMjQQErof"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
